
Дедлайн: 10.05.2015 23:59 (дедлайн жёсткий)

Что необходимо сделать: выполнить мини-исследование и предоставить отчёт. Для написания отчёта используйте ноутбук `template.ipynb`, лежащий рядом с текущим ридми

## Формат сдачи

Формат сдачи - публичный гитхаб репозиторий, содержащий файлы проекта. Проект можно выполнять в парах


## Структура репозитория


```
├── README.md          <- Файл, содержащий информацию об авторах проекта, его суть и необходимые шаги по воспроизведению вашего исследования 
├── requirements.txt   <- Файл, содержащий все необходимые зависимости
├── notebooks 		  <- Директория с вашими .ipynb ноутбуками
|   ├─ ...
├── scripts            <- Директория с вашими .py скриптами
|   ├─ ...
├── data               <- Директория со всеми скачанными файлами/датасетами
|   ├─ ...
├── dashboard          <- Здесь лежат все файлы, связанные с вашим дэшбордом
|   ├─ ...
```


## Структура исследования

Ваше исследование должно содержать 4 базовых этапа:

- Сбор данных
- Очистка данных и подготовка к анализу
- Анализ и визуализация
- Формирование отчёта

Придумайте цель/задачу для своего исследования, т.к. это будет влиять на то, что именно вы будете делать на каждом этапе

#### [Пример: исследование яндекса про русский рэп](https://yandex.ru/company/researches/2018/rap):

Исследование само по себе довольно дескриптивно - что из себя представляет русский рэп в смысле песен, из которых он состоит

- Здесь на этапе сбора данных собираются тексты песен
- На этапе очистки и подготовки к анализу из них, например, можно сделать датасет формата:

| author | song_name | lyrics | release_year |
| -------| --------  |  ------| ------------ | 
| ...... | .......   | ....... | ........... | 

- На этапе анализа данных из текстов достаются слова, приводятся к своей начальной форме, далее выполняется частотный анализ слов; авторы также смотрят на то, какие слова для какого автора наиболее характерны. Это сопровождается визуализациями (облака слов, стилизованные таблицы)
- Здесь вместо отчёта полноценная статья

Вы можете (например):
- достать данные с маркетплейса Х и изучить отзывы на товары в категориях Y и Z (и сравнить их)
- достать цены на авиа или ж/д билеты, чтобы посмотреть на сезонные колебания цен (или посмотреть на то, какие направления самые дешёвые); конкретно здесь визуализацией может выступать даже карта (придётся погуглить)
- проанализировать комментарии на видео-хостинге под видео на популярном канале (здесь как раз можно попробовать поэкспериментировать с облаками слов)

Если не уверены, подходит ли ваша идея - пишите! Ниже более подробный разбор этапов

### Сбор данных

На данном этапе вам необходимо собрать данные с любого источника, не предоставляющего их в готовом формате (т.е., например, json, csv, xlsx).

Для сбора данных можно использовать requests/Selenium, для парсинга - BeautifulSoup/Selenium (или другие подходящие библиотеки, требований по инструментам нет, однако не забудьте указать их в requirements.txt).

**Примечание:** Если вы используете готовый датасет, баллы за данный этап не начисляются.

### Очистка данных и подготовка к анализу

На этом этапе вам нужно провести следующие шаги:

- Подготовить данные к анализу (приведите данные к табличной форме, убедитесь, что все типы соответствуют необходимым, и т.д.)
- Выполнить минимальный разведывательный анализ данных (EDA), включающий:
	- описание типов переменных
	- вывод описательных статистик
	- уникальные значения/наиболее частые значения

на этом шаге этапе построения графиков может быть полезно для визуальной оценки особенности данных

### Анализ и визуализация

Этот этап предполагает выполнение основного анализа и создание сопровождающих визуализаций. Например, выявление трендов, закономерностей, корреляцией и т.д.

Создайте как минимум 3 **разные**  (по типу) визуализации (например, графики на основе matplotlib, seaborn, plotly или чего-то другого - облаков слов [например, wordcloud] / карт [например, folium] / etc.)

Визуализации должны быть полностью оформлены (подписи, заголовки, легенды там, где это необходимо)

### Формирование отчёта 

Сформулируйте ключевые выводы по результатам вашего исследования

### Дэшборд (опционально)

Если вы решите делать дэшборд, он должен отражать ключевые этапы проекта и предоставлять возможность “поиграться” с данными. Рекомендуемая структура:

#### 1. Навигация  
- **Главная страница** с заголовком проекта и кратким описанием источника данных.  
- Боковое или верхнее меню с разделами:
  - **Данные**  
  - **EDA (первичный анализ)**  
  - **Тренды & закономерности**  
  - **Выводы & рекомендации**


---

#### 2. Раздел «Данные»  
- Интерактивная таблица (`st.dataframe` или `dash_table.DataTable`, если говорить про streamlit vs. Dash) с возможностью фильтрации/поиска по колонкам
- Счётчики:
  - Общее число записей  
  - Количество пропусков  
  - Распределение по основным категориям (pie‑chart / bar‑chart / гистограммы). Не забывайте про оформление

#### 3. Раздел «EDA (первичный анализ)»  

Здесь лежат визуализации и всяческие статистические характеристики ваших данных

#### 4. Тренды & закономерности

Здесь хочется иметь доступ к следующим элементам:
- Фильтры для интерактивного выбора диапазона дат, категорий или других признаков (т.е. чтобы немного поменять ваши данные, но оставить логику)
- Возможность отрисовать визуализации по вашим данным

#### 5. Раздел «Выводы & рекомендации

Сюда добавьте текстовые карточки или markdown-блоки с ключевыми инсайтами и блок с рекомендациями + дальнейшими шагами (что можно улучшить, куда развивать анализ; подробно описано в `template.ipynb` в конце)

## Критерии оценивания проекта (20 баллов всего)

Максимальный балл за проект - 20 баллов. Если вы набираете больше, ваш итоговый балл = $min(\text{ваш балл}, 20)$

| №  | Этап / Критерий                                                      | Баллы  |
|----|----------------------------------------------------------------------|-------:|
| 1  | **Сбор данных (веб‑скрэпинг)**<br>– Собрать датасет на по крайней мере 100 записей с сервиса, не предоставляющего готовых файлов (т.е. не использовать условные csv/json как источник данных<br>| 4      |
| 2  | **Очистка и подготовка данных**<br>– Удаление пропусков и аномалий<br>– Приведение к `pandas.DataFrame` (вы можете работать и с другими структурами, но использование в проекте датафреймов из pandas и последующая работа с ними - требование)<br>– EDA: описательные статистики, количество пропусков, выбросов, уникальных значений, `.head()` | 4      |
| 3  | **Анализ данных и инсайты**<br>– Выявление ключевых трендов, закономерностей и взаимосвязей<br>– Обоснованные выводы на основе анализа. Постарайтесь сделать что-то больше, чем констатация описательных статистик - если вы видите закономерность, напишите, почему так может быть| 4      |
| 4  | **Визуализация**<br>– Не менее 3 различных графиков <br>– Оформление: заголовки, подписи осей, легенды и прочее выполнены верно| 4      |
| 5  | **Качество кода и репозиторий**<br>– Структура GitHub‑репозитория соблюдена, есть понятный README <br>– есть `requirements.txt`<br>– Код работает без ошибок, ваше исследование воспроизводится| 2      |
| 6  | **Наличие дэшборда**<br>– Реализован на Streamlit/Dash/Voila или аналогах<br>| 2      |
|    | **Итого**                                                             | **20**  |
